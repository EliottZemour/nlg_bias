{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from prettytable import PrettyTable\n",
    "import operator\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_diff(values):\n",
    "    # return the average pairwise different between all the pairs in the list\n",
    "    pairs = [(a, b) for idx, a in enumerate(values) for b in values[idx + 1:]]\n",
    "    difference = list(map(lambda x: abs(x[0] - x[1]), pairs))\n",
    "    return sum(difference)/len(difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(score_dir, undesired_subgroups):\n",
    "    path_to_dir = os.path.relpath(score_dir)\n",
    "    csv_files = [pos_csv for pos_csv in os.listdir(path_to_dir) if pos_csv.endswith('.csv')]\n",
    "    df = pd.concat([pd.read_csv(os.path.join(path_to_dir, file)) for file in csv_files])\n",
    "    df = df[~df['subgroup'].isin(undesired_subgroups)]\n",
    "\n",
    "    scores = []\n",
    "    groups = df['group'].unique().tolist()\n",
    "    table = PrettyTable([\"Domain\", \"Model\", \"Positive\", \"Neutral\", \"Negative\", \"Average\", \"Toxicity Ratio\"])\n",
    "    # toxicityTable = PrettyTable([\"Domain\", \"Model\", \"Ratio\"])\n",
    "\n",
    "    for group in groups:\n",
    "        # group = 'political_ideology'\n",
    "        data = df[df['group'] == group]\n",
    "        model_name = data[data['metric'] ==\n",
    "                        'regard-positive']['model'].values.tolist()\n",
    "        subgroup = data[data['metric'] ==\n",
    "                        'regard-positive']['subgroup'].values.tolist()\n",
    "        labels = [model_name[i] + '\\n' + subgroup[i]\n",
    "                for i in range(len(model_name))]\n",
    "\n",
    "        model_name = data[data['metric'] ==\n",
    "                        'regard-positive']['model'].unique().tolist()\n",
    "        subgroup = data[data['metric'] ==\n",
    "                        'regard-positive']['subgroup'].unique().tolist()\n",
    "\n",
    "        positive_regards = np.array(\n",
    "            data[data['metric'] == 'regard-positive']['score'].values.tolist())\n",
    "        negative_regards = np.array(\n",
    "            data[data['metric'] == 'regard-negative']['score'].values.tolist())\n",
    "        neutral_regards = np.array(\n",
    "            data[data['metric'] == 'regard-neutral']['score'].values.tolist())\n",
    "        toxicity = np.array(\n",
    "            data[data['metric'] == 'toxicity-ratio']['score'].values.tolist())\n",
    "\n",
    "        n_subgroups = len(subgroup)\n",
    "        for i in range(len(model_name)):\n",
    "            start_ind, end_ind = n_subgroups * i, n_subgroups * (i+1)\n",
    "            positive = positive_regards[start_ind:end_ind]\n",
    "            negative = negative_regards[start_ind:end_ind]\n",
    "            neutral = neutral_regards[start_ind:end_ind]\n",
    "            toxic = toxicity[start_ind:end_ind]\n",
    "\n",
    "            table.add_row(\n",
    "                [group, model_name[i], round(avg_diff(positive), 4), round(\n",
    "                avg_diff(neutral), 4), round(avg_diff(negative), 4), round(np.mean([avg_diff(positive),\n",
    "                avg_diff(neutral),avg_diff(negative)]), 4), round(avg_diff(toxic), 4)])\n",
    "\n",
    "            scores.append({'model': model_name[i], 'group': group,  'positive': round(avg_diff(positive), 4),\n",
    "                        'negative': round(avg_diff(negative), 4), 'neutral': round(avg_diff(neutral), 4), 'toxicity_ratio': round(avg_diff(toxic), 4)})\n",
    "    \n",
    "    return table, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dir = 'results/dexperts_gpt2_med_alpha1/score/'\n",
    "undesired_subgroups = ['Asian_Americans', 'Hispanic_and_Latino_Americans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mTRIGGER-GPT2\u001b[0m\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       Domain       | Model | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       gender       |  gpt2 |  0.0052  |  0.0154 |  0.0073  |  0.0093 |     0.0029     |\n",
      "|        race        |  gpt2 |  0.028   |  0.0152 |  0.0112  |  0.0181 |     0.0001     |\n",
      "| religious_ideology |  gpt2 |  0.0479  |  0.0228 |  0.0948  |  0.0552 |     0.0538     |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "directories = [\n",
    "    # 'results/new_temperature/gpt2/',\n",
    "    'results/new_temperature/trigger-gpt2/',\n",
    "    # 'results/new_temperature/dexperts_gpt2_temp1_alpha2/',\n",
    "    # 'results/new_temperature/dexperts_gpt2_antionly_temp1_alpha2/',\n",
    "]\n",
    "\n",
    "for score_dir in directories:\n",
    "    score_dir = os.path.join(score_dir, 'score/')\n",
    "    undesired_subgroups = ['Asian_Americans', 'Hispanic_and_Latino_Americans', 'hinduism', 'buddhism', 'sikhism', 'atheism']\n",
    "    table, scores = get_results(score_dir, undesired_subgroups)\n",
    "    print(f\"{Fore.LIGHTRED_EX}{score_dir.split('/')[-3].upper()}{Style.RESET_ALL}\")\n",
    "    print(table.get_string(sort_key=operator.itemgetter(1, 0), sortby=\"Domain\"), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mGPT2\u001b[0m\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       Domain       | Model | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       gender       |  gpt2 |  0.0032  |   0.01  |  0.0036  |  0.0056 |     0.0019     |\n",
      "|        race        |  gpt2 |  0.0308  |  0.0074 |  0.0231  |  0.0204 |     0.0013     |\n",
      "| religious_ideology |  gpt2 |  0.0461  |  0.0561 |  0.1229  |  0.075  |     0.0501     |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mDEXPERTS_GPT2_ANTIONLY_TEMP1_ALPHA05\u001b[0m\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|       gpt2_gender       | dexperts_gpt2 |  0.0011  |  0.0108 |  0.0119  |  0.0079 |     0.0017     |\n",
      "|        gpt2_race        | dexperts_gpt2 |  0.0152  |  0.009  |  0.0245  |  0.0162 |     0.0002     |\n",
      "| gpt2_religious_ideology | dexperts_gpt2 |  0.0554  |  0.0573 |  0.1121  |  0.075  |     0.0329     |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mDEXPERTS_GPT2_ANTIONLY_TEMP1_ALPHA1\u001b[0m\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|       gpt2_gender       | dexperts_gpt2 |  0.0021  |  0.0116 |  0.0081  |  0.0073 |     0.0011     |\n",
      "|        gpt2_race        | dexperts_gpt2 |  0.0036  |  0.0123 |  0.0168  |  0.0109 |     0.0006     |\n",
      "| gpt2_religious_ideology | dexperts_gpt2 |  0.0454  |  0.0673 |  0.1008  |  0.0712 |     0.0335     |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mDEXPERTS_GPT2_ANTIONLY_TEMP1_ALPHA2\u001b[0m\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|       gpt2_gender       | dexperts_gpt2 |  0.0173  |  0.0097 |  0.0106  |  0.0125 |     0.0005     |\n",
      "|        gpt2_race        | dexperts_gpt2 |  0.0051  |  0.0103 |  0.0208  |  0.0121 |     0.0004     |\n",
      "| gpt2_religious_ideology | dexperts_gpt2 |  0.0462  |  0.0647 |  0.078   |  0.063  |     0.0342     |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "directories = [\n",
    "    'results/new_temperature/gpt2/',\n",
    "    'results/new_temperature/dexperts_gpt2_antionly_temp1_alpha05/',\n",
    "    'results/new_temperature/dexperts_gpt2_antionly_temp1_alpha1/',\n",
    "    'results/new_temperature/dexperts_gpt2_antionly_temp1_alpha2/',\n",
    "]\n",
    "tables = []\n",
    "\n",
    "for score_dir in directories:\n",
    "    score_dir = os.path.join(score_dir, 'score/')\n",
    "    undesired_subgroups = ['Asian_Americans', 'Hispanic_and_Latino_Americans', 'hinduism', 'buddhism', 'sikhism', 'atheism']\n",
    "    table, scores = get_results(score_dir, undesired_subgroups)\n",
    "    tables.append(table)\n",
    "    print(f\"{Fore.LIGHTRED_EX}{score_dir.split('/')[-3].upper()}{Style.RESET_ALL}\")\n",
    "    print(table.get_string(sort_key=operator.itemgetter(1, 0), sortby=\"Domain\"), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Domain</th>\n",
       "            <th>Model</th>\n",
       "            <th>Positive</th>\n",
       "            <th>Neutral</th>\n",
       "            <th>Negative</th>\n",
       "            <th>Average</th>\n",
       "            <th>Toxicity Ratio</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>gpt2_race</td>\n",
       "            <td>dexperts_gpt2</td>\n",
       "            <td>0.0051</td>\n",
       "            <td>0.0103</td>\n",
       "            <td>0.0208</td>\n",
       "            <td>0.0121</td>\n",
       "            <td>0.0004</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>gpt2_gender</td>\n",
       "            <td>dexperts_gpt2</td>\n",
       "            <td>0.0173</td>\n",
       "            <td>0.0097</td>\n",
       "            <td>0.0106</td>\n",
       "            <td>0.0125</td>\n",
       "            <td>0.0005</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>gpt2_religious_ideology</td>\n",
       "            <td>dexperts_gpt2</td>\n",
       "            <td>0.0462</td>\n",
       "            <td>0.0647</td>\n",
       "            <td>0.078</td>\n",
       "            <td>0.063</td>\n",
       "            <td>0.0342</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
       "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
       "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
       "|        gpt2_race        | dexperts_gpt2 |  0.0051  |  0.0103 |  0.0208  |  0.0121 |     0.0004     |\n",
       "|       gpt2_gender       | dexperts_gpt2 |  0.0173  |  0.0097 |  0.0106  |  0.0125 |     0.0005     |\n",
       "| gpt2_religious_ideology | dexperts_gpt2 |  0.0462  |  0.0647 |  0.078   |  0.063  |     0.0342     |\n",
       "+-------------------------+---------------+----------+---------+----------+---------+----------------+"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mGPT2\u001b[0m\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       Domain       | Model | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       gender       |  gpt2 |  0.0032  |   0.01  |  0.0036  |  0.0056 |     0.0019     |\n",
      "|        race        |  gpt2 |  0.0308  |  0.0074 |  0.0231  |  0.0204 |     0.0013     |\n",
      "| religious_ideology |  gpt2 |  0.0461  |  0.0561 |  0.1229  |  0.075  |     0.0501     |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mDEXPERTS_GPT2_TEMP1_ALPHA05\u001b[0m\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|       gpt2_gender       | dexperts_gpt2 |  0.0035  |  0.0126 |  0.0079  |  0.008  |     0.0026     |\n",
      "|        gpt2_race        | dexperts_gpt2 |  0.0231  |  0.0027 |  0.026   |  0.0173 |     0.0021     |\n",
      "| gpt2_religious_ideology | dexperts_gpt2 |  0.0483  |  0.0235 |  0.1008  |  0.0575 |     0.042      |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mDEXPERTS_GPT2_TEMP1_ALPHA1\u001b[0m\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|       gpt2_gender       | dexperts_gpt2 |  0.0082  |  0.0176 |  0.0101  |  0.012  |     0.0026     |\n",
      "|        gpt2_race        | dexperts_gpt2 |  0.0234  |  0.004  |  0.0266  |  0.018  |     0.0008     |\n",
      "| gpt2_religious_ideology | dexperts_gpt2 |  0.024   |  0.047  |  0.0557  |  0.0422 |     0.042      |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mDEXPERTS_GPT2_TEMP1_ALPHA2\u001b[0m\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|       gpt2_gender       | dexperts_gpt2 |  0.0047  |  0.0158 |  0.012   |  0.0108 |     0.0003     |\n",
      "|        gpt2_race        | dexperts_gpt2 |  0.0185  |  0.0044 |  0.022   |  0.015  |     0.0014     |\n",
      "| gpt2_religious_ideology | dexperts_gpt2 |  0.0371  |  0.0386 |  0.0323  |  0.036  |     0.1061     |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "directories = [\n",
    "    'results/new_temperature/gpt2/',\n",
    "    'results/new_temperature/dexperts_gpt2_temp1_alpha05/',\n",
    "    'results/new_temperature/dexperts_gpt2_temp1_alpha1/',\n",
    "    'results/new_temperature/dexperts_gpt2_temp1_alpha2/',\n",
    "]\n",
    "tables = []\n",
    "for score_dir in directories:\n",
    "    score_dir = os.path.join(score_dir, 'score/')\n",
    "    undesired_subgroups = ['Asian_Americans', 'Hispanic_and_Latino_Americans', 'hinduism', 'buddhism', 'sikhism', 'atheism']\n",
    "    table, scores = get_results(score_dir, undesired_subgroups)\n",
    "    tables.append(table)\n",
    "    print(f\"{Fore.LIGHTRED_EX}{score_dir.split('/')[-3].upper()}{Style.RESET_ALL}\")\n",
    "    print(table.get_string(sort_key=operator.itemgetter(1, 0), sortby=\"Domain\"), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <thead>\n",
       "        <tr>\n",
       "            <th>Domain</th>\n",
       "            <th>Model</th>\n",
       "            <th>Positive</th>\n",
       "            <th>Neutral</th>\n",
       "            <th>Negative</th>\n",
       "            <th>Average</th>\n",
       "            <th>Toxicity Ratio</th>\n",
       "        </tr>\n",
       "    </thead>\n",
       "    <tbody>\n",
       "        <tr>\n",
       "            <td>gpt2_race</td>\n",
       "            <td>dexperts_gpt2</td>\n",
       "            <td>0.0185</td>\n",
       "            <td>0.0044</td>\n",
       "            <td>0.022</td>\n",
       "            <td>0.015</td>\n",
       "            <td>0.0014</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>gpt2_gender</td>\n",
       "            <td>dexperts_gpt2</td>\n",
       "            <td>0.0047</td>\n",
       "            <td>0.0158</td>\n",
       "            <td>0.012</td>\n",
       "            <td>0.0108</td>\n",
       "            <td>0.0003</td>\n",
       "        </tr>\n",
       "        <tr>\n",
       "            <td>gpt2_religious_ideology</td>\n",
       "            <td>dexperts_gpt2</td>\n",
       "            <td>0.0371</td>\n",
       "            <td>0.0386</td>\n",
       "            <td>0.0323</td>\n",
       "            <td>0.036</td>\n",
       "            <td>0.1061</td>\n",
       "        </tr>\n",
       "    </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
       "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
       "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
       "|        gpt2_race        | dexperts_gpt2 |  0.0185  |  0.0044 |  0.022   |  0.015  |     0.0014     |\n",
       "|       gpt2_gender       | dexperts_gpt2 |  0.0047  |  0.0158 |  0.012   |  0.0108 |     0.0003     |\n",
       "| gpt2_religious_ideology | dexperts_gpt2 |  0.0371  |  0.0386 |  0.0323  |  0.036  |     0.1061     |\n",
       "+-------------------------+---------------+----------+---------+----------+---------+----------------+"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tables[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e8cf6c8a0e336b88b8b74ceca74e60238c1300ebcacd360aa408440a347c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
