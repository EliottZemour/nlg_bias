{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from prettytable import PrettyTable\n",
    "import operator\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_diff(values):\n",
    "    # return the average pairwise different between all the pairs in the list\n",
    "    pairs = [(a, b) for idx, a in enumerate(values) for b in values[idx + 1:]]\n",
    "    difference = list(map(lambda x: abs(x[0] - x[1]), pairs))\n",
    "    return sum(difference)/len(difference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(score_dir, undesired_subgroups):\n",
    "    path_to_dir = os.path.relpath(score_dir)\n",
    "    csv_files = [pos_csv for pos_csv in os.listdir(path_to_dir) if pos_csv.endswith('.csv')]\n",
    "    df = pd.concat([pd.read_csv(os.path.join(path_to_dir, file)) for file in csv_files])\n",
    "    df = df[~df['subgroup'].isin(undesired_subgroups)]\n",
    "\n",
    "    scores = []\n",
    "    groups = df['group'].unique().tolist()\n",
    "    table = PrettyTable([\"Domain\", \"Model\", \"Positive\", \"Neutral\", \"Negative\", \"Average\", \"Toxicity Ratio\"])\n",
    "    # toxicityTable = PrettyTable([\"Domain\", \"Model\", \"Ratio\"])\n",
    "\n",
    "    for group in groups:\n",
    "        # group = 'political_ideology'\n",
    "        data = df[df['group'] == group]\n",
    "        model_name = data[data['metric'] ==\n",
    "                        'regard-positive']['model'].values.tolist()\n",
    "        subgroup = data[data['metric'] ==\n",
    "                        'regard-positive']['subgroup'].values.tolist()\n",
    "        labels = [model_name[i] + '\\n' + subgroup[i]\n",
    "                for i in range(len(model_name))]\n",
    "\n",
    "        model_name = data[data['metric'] ==\n",
    "                        'regard-positive']['model'].unique().tolist()\n",
    "        subgroup = data[data['metric'] ==\n",
    "                        'regard-positive']['subgroup'].unique().tolist()\n",
    "\n",
    "        positive_regards = np.array(\n",
    "            data[data['metric'] == 'regard-positive']['score'].values.tolist())\n",
    "        negative_regards = np.array(\n",
    "            data[data['metric'] == 'regard-negative']['score'].values.tolist())\n",
    "        neutral_regards = np.array(\n",
    "            data[data['metric'] == 'regard-neutral']['score'].values.tolist())\n",
    "        toxicity = np.array(\n",
    "            data[data['metric'] == 'toxicity-ratio']['score'].values.tolist())\n",
    "\n",
    "        n_subgroups = len(subgroup)\n",
    "        for i in range(len(model_name)):\n",
    "            start_ind, end_ind = n_subgroups * i, n_subgroups * (i+1)\n",
    "            positive = positive_regards[start_ind:end_ind]\n",
    "            negative = negative_regards[start_ind:end_ind]\n",
    "            neutral = neutral_regards[start_ind:end_ind]\n",
    "            toxic = toxicity[start_ind:end_ind]\n",
    "\n",
    "            table.add_row(\n",
    "                [group, model_name[i], round(avg_diff(positive), 4), round(\n",
    "                avg_diff(neutral), 4), round(avg_diff(negative), 4), round(np.mean([avg_diff(positive),\n",
    "                avg_diff(neutral),avg_diff(negative)]), 4), round(avg_diff(toxic), 4)])\n",
    "\n",
    "            scores.append({'model': model_name[i], 'group': group,  'positive': round(avg_diff(positive), 4),\n",
    "                        'negative': round(avg_diff(negative), 4), 'neutral': round(avg_diff(neutral), 4), 'toxicity_ratio': round(avg_diff(toxic), 4)})\n",
    "    \n",
    "    return table, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_dir = 'results/dexperts_gpt2_med_alpha1/score/'\n",
    "undesired_subgroups = ['Asian_Americans', 'Hispanic_and_Latino_Americans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mGPT2\u001b[0m\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       Domain       | Model | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       gender       |  gpt2 |  0.0032  |   0.01  |  0.0036  |  0.0056 |     0.0019     |\n",
      "|        race        |  gpt2 |  0.0308  |  0.0074 |  0.0231  |  0.0204 |     0.0013     |\n",
      "| religious_ideology |  gpt2 |  0.0461  |  0.0561 |  0.1229  |  0.075  |     0.0501     |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mTRIGGER-GPT2\u001b[0m\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       Domain       | Model | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       gender       |  gpt2 |  0.0052  |  0.0154 |  0.0073  |  0.0093 |     0.0029     |\n",
      "|        race        |  gpt2 |  0.028   |  0.0152 |  0.0112  |  0.0181 |     0.0001     |\n",
      "| religious_ideology |  gpt2 |  0.0479  |  0.0228 |  0.0948  |  0.0552 |     0.0538     |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mDEXPERTS_GPT2_ANTIONLY_TEMP1_ALPHA2\u001b[0m\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|       gpt2_gender       | dexperts_gpt2 |  0.0173  |  0.0097 |  0.0106  |  0.0125 |     0.0005     |\n",
      "|        gpt2_race        | dexperts_gpt2 |  0.0051  |  0.0103 |  0.0208  |  0.0121 |     0.0004     |\n",
      "| gpt2_religious_ideology | dexperts_gpt2 |  0.0462  |  0.0647 |  0.078   |  0.063  |     0.0342     |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "directories = [\n",
    "    'results/new_temperature/gpt2/',\n",
    "    'results/new_temperature/trigger-gpt2/',\n",
    "    'results/new_temperature/dexperts_gpt2_antionly_temp1_alpha2/',\n",
    "]\n",
    "\n",
    "for score_dir in directories:\n",
    "    score_dir = os.path.join(score_dir, 'score/')\n",
    "    undesired_subgroups = ['Asian_Americans', 'Hispanic_and_Latino_Americans', 'hinduism', 'buddhism', 'sikhism', 'atheism']\n",
    "    table, scores = get_results(score_dir, undesired_subgroups)\n",
    "    print(f\"{Fore.LIGHTRED_EX}{score_dir.split('/')[-3].upper()}{Style.RESET_ALL}\")\n",
    "    print(table.get_string(sort_key=operator.itemgetter(1, 0), sortby=\"Domain\"), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mGPT2\u001b[0m\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       Domain       | Model | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       gender       |  gpt2 |  0.0032  |   0.01  |  0.0036  |  0.0056 |     0.0019     |\n",
      "|        race        |  gpt2 |  0.0483  |  0.028  |  0.0188  |  0.0317 |     0.0016     |\n",
      "| religious_ideology |  gpt2 |  0.1449  |  0.0709 |  0.165   |  0.1269 |     0.066      |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mTRIGGER-GPT2\u001b[0m\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       Domain       | Model | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+\n",
      "|       gender       |  gpt2 |  0.0052  |  0.0154 |  0.0073  |  0.0093 |     0.0029     |\n",
      "|        race        |  gpt2 |  0.0251  |   0.01  |  0.0253  |  0.0201 |     0.0003     |\n",
      "| religious_ideology |  gpt2 |  0.1729  |  0.0523 |  0.1376  |  0.121  |     0.0774     |\n",
      "+--------------------+-------+----------+---------+----------+---------+----------------+ \n",
      "\n",
      "\u001b[91mDEXPERTS_GPT2_ANTIONLY_TEMP1_ALPHA2\u001b[0m\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|          Domain         |     Model     | Positive | Neutral | Negative | Average | Toxicity Ratio |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+\n",
      "|       gpt2_gender       | dexperts_gpt2 |  0.0173  |  0.0097 |  0.0106  |  0.0125 |     0.0005     |\n",
      "|        gpt2_race        | dexperts_gpt2 |  0.0109  |  0.0164 |  0.0162  |  0.0145 |     0.0011     |\n",
      "| gpt2_religious_ideology | dexperts_gpt2 |  0.2034  |  0.0754 |  0.137   |  0.1386 |     0.0359     |\n",
      "+-------------------------+---------------+----------+---------+----------+---------+----------------+ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "directories = [\n",
    "    'results/new_temperature/gpt2/',\n",
    "    'results/new_temperature/trigger-gpt2/',\n",
    "    'results/new_temperature/dexperts_gpt2_antionly_temp1_alpha2/',\n",
    "]\n",
    "\n",
    "for score_dir in directories:\n",
    "    score_dir = os.path.join(score_dir, 'score/')\n",
    "    undesired_subgroups = [] #['Asian_Americans', 'Hispanic_and_Latino_Americans', 'hinduism', 'buddhism', 'sikhism', 'atheism']\n",
    "    table, scores = get_results(score_dir, undesired_subgroups)\n",
    "    print(f\"{Fore.LIGHTRED_EX}{score_dir.split('/')[-3].upper()}{Style.RESET_ALL}\")\n",
    "    print(table.get_string(sort_key=operator.itemgetter(1, 0), sortby=\"Domain\"), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e2e8cf6c8a0e336b88b8b74ceca74e60238c1300ebcacd360aa408440a347c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
