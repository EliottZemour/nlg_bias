{
  "base_model": "gpt2-large",
  "dataset": "wikitext-2-v1",
  "dexperts": {
    "expert_model": null,
    "antiexpert_model": null,
    "alpha": 2.0
  },
  "stride": -1,
  "max_length": -1,
  "max_length_pattern": 32,
  "save_json": "perplexity/results_gpt2_large.json",
  "load_json": "perplexity_args.json",
  "perplexity": 17.32334327697754
}